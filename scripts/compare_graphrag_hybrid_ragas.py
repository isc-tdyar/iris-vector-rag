#!/usr/bin/env python3
"""
RAGAS Comparison: GraphRAG vs HybridGraphRAG

This script uses the existing RAGAS evaluation framework to demonstrate
the performance improvements achieved by HybridGraphRAG with iris_graph_core.

Key features:
- Uses established RAGAS metrics framework
- Side-by-side comparison of GraphRAG vs HybridGraphRAG
- Statistical significance testing
- Performance timing analysis
- Automated capability detection (iris_graph_core availability)

Results show both RAGAS quality metrics and performance improvements.
"""

import logging
import os
import sys
import time
from pathlib import Path
from typing import Any, Dict, List

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from evaluation_framework.ragas_metrics_framework import BiomedicalRAGASFramework
from scripts.data_loaders.pmc_loader import SAMPLE_QUERIES

# Import RAGAS framework
from scripts.generate_ragas_evaluation import (
    EvaluationConfig,
    RAGASEvaluationOrchestrator,
)

logger = logging.getLogger(__name__)


class GraphRAGHybridComparison:
    """
    Compare GraphRAG vs HybridGraphRAG using RAGAS evaluation framework.
    """

    def __init__(self):
        """Initialize comparison with focused configuration"""
        self.output_dir = Path("outputs/graphrag_hybrid_comparison")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )

        # Check iris_graph_core availability
        self.iris_core_available = self._check_iris_graph_core()

    def _check_iris_graph_core(self) -> bool:
        """Check if iris_graph_core is available for HybridGraphRAG"""
        try:
            from iris_vector_rag.config.manager import ConfigurationManager
            from iris_vector_rag.core.connection import ConnectionManager
            from iris_vector_rag.pipelines.hybrid_graphrag import HybridGraphRAGPipeline

            # Try to initialize and check iris_engine availability
            conn_mgr = ConnectionManager()
            config_mgr = ConfigurationManager()
            pipeline = HybridGraphRAGPipeline(conn_mgr, config_mgr)

            return pipeline.iris_engine is not None
        except Exception as e:
            logger.warning(f"iris_graph_core not available: {e}")
            return False

    def create_focused_config(self, num_queries: int = 10) -> EvaluationConfig:
        """Create configuration focused on GraphRAG comparison"""
        pipelines = ["GraphRAG"]

        if self.iris_core_available:
            pipelines.append("HybridGraphRAG")
            logger.info("âœ… iris_graph_core available - full comparison enabled")
        else:
            logger.info("âš ï¸  iris_graph_core not available - baseline GraphRAG only")
            logger.info(
                "   Place graph-ai project adjacent to rag-templates for enhanced features"
            )

        return EvaluationConfig(
            num_queries=num_queries,
            pipelines=pipelines,
            output_dir=self.output_dir,
            use_cache=True,
            parallel_execution=False,  # Sequential for timing comparison
            confidence_level=0.95,
            target_accuracy=0.75,  # Reasonable target for comparison
        )

    def run_performance_timing_analysis(self, num_queries: int = 5) -> Dict[str, Any]:
        """Run detailed performance timing analysis"""
        logger.info("ðŸ• Running performance timing analysis...")

        # Import pipelines
        from iris_vector_rag.config.manager import ConfigurationManager
        from iris_vector_rag.core.connection import ConnectionManager
        from iris_vector_rag.pipelines.graphrag import GraphRAGPipeline

        conn_mgr = ConnectionManager()
        config_mgr = ConfigurationManager()

        timing_results = {
            "GraphRAG": {"times": [], "avg_time": 0, "available": True},
            "HybridGraphRAG": {"times": [], "avg_time": 0, "available": False},
        }

        # Test GraphRAG
        try:
            graphrag = GraphRAGPipeline(conn_mgr, config_mgr)
            test_queries = SAMPLE_QUERIES[:num_queries]

            for query in test_queries:
                start_time = time.perf_counter()
                try:
                    result = graphrag.query(
                        query, method="kg", top_k=5, generate_answer=False
                    )
                    end_time = time.perf_counter()
                    timing_results["GraphRAG"]["times"].append(
                        (end_time - start_time) * 1000
                    )
                except Exception as e:
                    logger.warning(f"GraphRAG query failed: {e}")
                    timing_results["GraphRAG"]["times"].append(0)

            valid_times = [t for t in timing_results["GraphRAG"]["times"] if t > 0]
            timing_results["GraphRAG"]["avg_time"] = (
                sum(valid_times) / len(valid_times) if valid_times else 0
            )

        except Exception as e:
            logger.error(f"GraphRAG initialization failed: {e}")
            timing_results["GraphRAG"]["available"] = False

        # Test HybridGraphRAG if available
        if self.iris_core_available:
            try:
                from iris_vector_rag.pipelines.hybrid_graphrag import HybridGraphRAGPipeline

                hybrid_graphrag = HybridGraphRAGPipeline(conn_mgr, config_mgr)
                timing_results["HybridGraphRAG"]["available"] = True

                # Test different methods
                methods_to_test = (
                    ["kg", "hybrid", "rrf", "vector"]
                    if hybrid_graphrag.iris_engine
                    else ["kg"]
                )
                method_timings = {}

                for method in methods_to_test:
                    method_times = []
                    for query in test_queries:
                        start_time = time.perf_counter()
                        try:
                            result = hybrid_graphrag.query(
                                query, method=method, top_k=5, generate_answer=False
                            )
                            end_time = time.perf_counter()
                            method_times.append((end_time - start_time) * 1000)
                        except Exception as e:
                            logger.warning(f"HybridGraphRAG {method} query failed: {e}")
                            method_times.append(0)

                    valid_method_times = [t for t in method_times if t > 0]
                    method_timings[method] = (
                        sum(valid_method_times) / len(valid_method_times)
                        if valid_method_times
                        else 0
                    )

                # Use best performing method for overall timing
                best_method = min(
                    method_timings.items(),
                    key=lambda x: x[1] if x[1] > 0 else float("inf"),
                )
                timing_results["HybridGraphRAG"]["avg_time"] = best_method[1]
                timing_results["HybridGraphRAG"]["best_method"] = best_method[0]
                timing_results["HybridGraphRAG"]["method_timings"] = method_timings

            except Exception as e:
                logger.error(f"HybridGraphRAG testing failed: {e}")

        return timing_results

    def run_ragas_comparison(self, num_queries: int = 10) -> Dict[str, Any]:
        """Run RAGAS evaluation comparison"""
        logger.info("ðŸ“Š Running RAGAS evaluation comparison...")

        config = self.create_focused_config(num_queries)
        orchestrator = RAGASEvaluationOrchestrator(config)

        return orchestrator.run_comprehensive_evaluation()

    def run_comprehensive_comparison(self, num_queries: int = 10) -> Dict[str, Any]:
        """Run comprehensive comparison including RAGAS and performance analysis"""
        logger.info("ðŸš€ Starting Comprehensive GraphRAG vs HybridGraphRAG Comparison")
        logger.info("=" * 80)

        start_time = time.time()

        # Performance timing analysis
        timing_results = self.run_performance_timing_analysis(min(num_queries, 5))

        # RAGAS evaluation comparison
        ragas_results = self.run_ragas_comparison(num_queries)

        # Combine results
        comprehensive_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "iris_graph_core_available": self.iris_core_available,
            "num_queries_tested": num_queries,
            "timing_analysis": timing_results,
            "ragas_evaluation": ragas_results,
            "summary": self._generate_comparison_summary(timing_results, ragas_results),
        }

        # Save comprehensive results
        results_file = (
            self.output_dir / f"comprehensive_comparison_{int(time.time())}.json"
        )
        import json

        with open(results_file, "w") as f:
            json.dump(comprehensive_results, f, indent=2, default=str)

        # Print summary
        self._print_comparison_summary(comprehensive_results)

        total_time = time.time() - start_time
        logger.info(f"Comparison completed in {total_time:.1f} seconds")
        logger.info(f"Detailed results saved to: {results_file}")

        return comprehensive_results

    def _generate_comparison_summary(
        self, timing_results: Dict[str, Any], ragas_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate comparison summary"""
        summary = {
            "performance_improvement": None,
            "ragas_improvement": None,
            "capabilities_comparison": {
                "GraphRAG": [
                    "entity_extraction",
                    "graph_traversal",
                    "basic_vector_search",
                ],
                "HybridGraphRAG": [
                    "entity_extraction",
                    "graph_traversal",
                    "basic_vector_search",
                ],
            },
        }

        # Performance comparison
        if (
            timing_results["GraphRAG"]["available"]
            and timing_results["HybridGraphRAG"]["available"]
            and timing_results["GraphRAG"]["avg_time"] > 0
            and timing_results["HybridGraphRAG"]["avg_time"] > 0
        ):

            improvement = (
                (
                    timing_results["GraphRAG"]["avg_time"]
                    - timing_results["HybridGraphRAG"]["avg_time"]
                )
                / timing_results["GraphRAG"]["avg_time"]
            ) * 100
            summary["performance_improvement"] = improvement

        # Enhanced capabilities if iris_graph_core available
        if self.iris_core_available:
            summary["capabilities_comparison"]["HybridGraphRAG"].extend(
                [
                    "rrf_fusion",
                    "hnsw_optimized_vectors",
                    "multi_modal_search",
                    "iris_ifind_text_search",
                    "adaptive_query_routing",
                ]
            )

        # RAGAS comparison
        pipeline_metrics = ragas_results.get("pipeline_metrics", {})
        if "GraphRAG" in pipeline_metrics and "HybridGraphRAG" in pipeline_metrics:
            graphrag_score = (
                pipeline_metrics["GraphRAG"]
                .get("answer_correctness", {})
                .get("mean", 0)
            )
            hybrid_score = (
                pipeline_metrics["HybridGraphRAG"]
                .get("answer_correctness", {})
                .get("mean", 0)
            )

            if graphrag_score > 0:
                ragas_improvement = (
                    (hybrid_score - graphrag_score) / graphrag_score
                ) * 100
                summary["ragas_improvement"] = ragas_improvement

        return summary

    def _print_comparison_summary(self, results: Dict[str, Any]):
        """Print comparison summary"""
        print("\n" + "=" * 80)
        print("ðŸ† GRAPHRAG VS HYBRIDGRAPHRAG COMPARISON SUMMARY")
        print("=" * 80)

        print(f"ðŸ“… Timestamp: {results['timestamp']}")
        print(
            f"ðŸ”§ IRIS Graph Core Available: {'âœ… Yes' if results['iris_graph_core_available'] else 'âŒ No'}"
        )
        print(f"ðŸ“Š Queries Tested: {results['num_queries_tested']}")

        # Performance Summary
        print(f"\nâš¡ PERFORMANCE ANALYSIS:")
        timing = results["timing_analysis"]
        if timing["GraphRAG"]["available"]:
            print(f"   GraphRAG Average: {timing['GraphRAG']['avg_time']:.1f}ms")
        if timing["HybridGraphRAG"]["available"]:
            print(
                f"   HybridGraphRAG Average: {timing['HybridGraphRAG']['avg_time']:.1f}ms"
            )
            if "method_timings" in timing["HybridGraphRAG"]:
                for method, time_ms in timing["HybridGraphRAG"][
                    "method_timings"
                ].items():
                    print(f"     - {method} method: {time_ms:.1f}ms")

        # Performance improvement
        summary = results["summary"]
        if summary["performance_improvement"] is not None:
            improvement = summary["performance_improvement"]
            if improvement > 0:
                print(f"   ðŸš€ Performance Improvement: {improvement:.1f}% faster")
            else:
                print(f"   âš ï¸  Performance Change: {abs(improvement):.1f}% slower")

        # RAGAS Quality Summary
        print(f"\nðŸ“ˆ RAGAS QUALITY METRICS:")
        ragas = results["ragas_evaluation"]
        pipeline_metrics = ragas.get("pipeline_metrics", {})

        for pipeline, metrics in pipeline_metrics.items():
            answer_correctness = metrics.get("answer_correctness", {}).get("mean", 0)
            print(f"   {pipeline}: {answer_correctness:.3f} answer correctness")

        if summary["ragas_improvement"] is not None:
            improvement = summary["ragas_improvement"]
            if improvement > 0:
                print(f"   ðŸ“Š Quality Improvement: {improvement:.1f}% better")
            else:
                print(f"   ðŸ“Š Quality Change: {abs(improvement):.1f}% different")

        # Capabilities Summary
        print(f"\nðŸ› ï¸  CAPABILITIES COMPARISON:")
        capabilities = summary["capabilities_comparison"]
        for pipeline, caps in capabilities.items():
            print(f"   {pipeline}: {', '.join(caps)}")

        # Recommendations
        print(f"\nðŸ’¡ RECOMMENDATIONS:")
        if results["iris_graph_core_available"]:
            print("   âœ… Use HybridGraphRAG for enhanced search capabilities")
            print("   âœ… Consider 'hybrid' or 'rrf' methods for best performance")
            print("   âœ… Upgrade to licensed IRIS for ACORN=1 optimization")
        else:
            print("   ðŸ”§ Place graph-ai project adjacent to rag-templates")
            print("   ðŸ”§ Install iris_graph_core for enhanced capabilities")
            print("   ðŸ“ˆ Current setup provides baseline GraphRAG functionality")

        print("=" * 80)


def main():
    """Run GraphRAG vs HybridGraphRAG comparison"""
    try:
        comparison = GraphRAGHybridComparison()

        # Get number of queries from environment or use default
        num_queries = int(os.getenv("COMPARISON_QUERIES", "10"))

        # Run comprehensive comparison
        results = comparison.run_comprehensive_comparison(num_queries)

        # Determine success based on availability and results
        success = (
            results["iris_graph_core_available"]
            or len(results["ragas_evaluation"]["pipeline_metrics"]) > 0
        )

        return 0 if success else 1

    except Exception as e:
        logger.error(f"Comparison failed: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
